#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ Qwen3-Omni-Flash å­—å¹•æå–å™¨
åŸºäº NotebookLM çŸ¥è¯†åº“ä¸­çš„æŠ€æœ¯æ–¹æ¡ˆå®ç°åˆ†ç‰‡æµæ°´çº¿ä¼˜åŒ–å’Œè§‚å¯Ÿå€¼æ©ç æœºåˆ¶
"""

import os
import cv2
import json
import requests
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('subtitle_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class SubtitleFrame:
    """å­—å¹•å¸§æ•°æ®ç»“æ„"""
    frame_path: str
    timestamp: float
    frame_index: int
    confidence: float = 0.0

@dataclass
class SubtitleSegment:
    """å­—å¹•ç‰‡æ®µæ•°æ®ç»“æ„"""
    start_time: float
    end_time: float
    text: str
    confidence: float = 0.0
    emotion_tags: List[str] = None

class EnhancedQwen3SubtitleExtractor:
    """å¢å¼ºç‰ˆ Qwen3-Omni-Flash å­—å¹•æå–å™¨"""
    
    def __init__(self, api_key: str, max_workers: int = 4):
        self.api_key = api_key
        self.base_url = "https://dashscope.aliyuncs.com/api/v1/chat/completions"
        self.max_workers = max_workers
        self.temp_dir = Path("temp")
        self.temp_dir.mkdir(exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
    def extract_subtitles_from_video(self, video_path: str, output_path: str, 
                                   task_id: str = None) -> Dict[str, Any]:
        """
        ä»è§†é¢‘ä¸­æå–å­—å¹•ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡º SRT æ–‡ä»¶è·¯å¾„
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºæ–‡ä»¶ç®¡ç†ï¼‰
            
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        try:
            self.logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
            
            # 1. åˆå§‹åŒ–ä»»åŠ¡ç›®å½•
            task_dir = self._init_task_directory(task_id)
            
            # 2. æå–è§†é¢‘å¸§ï¼ˆåˆ†ç‰‡æµæ°´çº¿ï¼‰
            frames = self._extract_frames_with_pipeline(video_path, task_dir)
            self.logger.info(f"ğŸ“¸ æå–äº† {len(frames)} å¸§")
            
            # 3. æ£€æµ‹å­—å¹•å¸§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            subtitle_frames = self._detect_subtitle_frames_enhanced(frames, task_dir)
            self.logger.info(f"ğŸ” æ£€æµ‹åˆ° {len(subtitle_frames)} å­—å¹•å¸§")
            
            # 4. ä½¿ç”¨ Qwen3 è¿›è¡Œ OCR è¯†åˆ«ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
            ocr_results = self._ocr_with_qwen3_parallel(subtitle_frames, task_dir)
            
            # 5. æƒ…æ„Ÿåˆ†æå’Œå­—å¹•ä¼˜åŒ–
            enhanced_subtitles = self._enhance_subtitles_with_emotion(ocr_results, task_dir)
            
            # 6. ç”Ÿæˆ SRT æ–‡ä»¶
            self._generate_enhanced_srt(enhanced_subtitles, output_path)
            
            # 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files(task_dir)
            
            result = {
                "success": True,
                "video_path": video_path,
                "output_path": output_path,
                "total_frames": len(frames),
                "subtitle_frames": len(subtitle_frames),
                "subtitles_count": len(enhanced_subtitles),
                "processing_time": time.time() - start_time,
                "task_id": task_id
            }
            
            self.logger.info(f"âœ… å­—å¹•æå–å®Œæˆ: {output_path}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ å­—å¹•æå–å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "video_path": video_path,
                "task_id": task_id
            }
    
    def _init_task_directory(self, task_id: str) -> Path:
        """åˆå§‹åŒ–ä»»åŠ¡ç›®å½•"""
        if task_id:
            task_dir = Path(f"tasks/{task_id}")
        else:
            task_dir = self.temp_dir / f"task_{int(time.time())}"
        
        task_dir.mkdir(parents=True, exist_ok=True)
        (task_dir / "frames").mkdir(exist_ok=True)
        (task_dir / "output").mkdir(exist_ok=True)
        
        return task_dir
    
    def _extract_frames_with_pipeline(self, video_path: str, task_dir: Path) -> List[str]:
        """åˆ†ç‰‡æµæ°´çº¿æå–è§†é¢‘å¸§"""
        frames = []
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        frame_count = 0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps_video = cap.get(cv2.CAP_PROP_FPS)
        
        self.logger.info(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps_video:.2f} FPS")
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # æŒ‰æŒ‡å®š fps é‡‡æ ·ï¼ˆä¼˜åŒ–ï¼šé™ä½é‡‡æ ·ç‡ä»¥æé«˜æ€§èƒ½ï¼‰
            sample_rate = max(1, int(fps_video / 2))  # æ¯2ç§’é‡‡æ ·1å¸§
            if frame_count % sample_rate == 0:
                frame_path = task_dir / "frames" / f"frame_{frame_count}.jpg"
                cv2.imwrite(str(frame_path), frame)
                frames.append(str(frame_path))
            
            frame_count += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if frame_count % 100 == 0:
                progress = (frame_count / total_frames) * 100
                self.logger.info(f"ğŸ“Š å¸§æå–è¿›åº¦: {progress:.1f}%")
        
        cap.release()
        return frames
    
    def _detect_subtitle_frames_enhanced(self, frames: List[str], task_dir: Path) -> List[SubtitleFrame]:
        """å¢å¼ºç‰ˆå­—å¹•å¸§æ£€æµ‹"""
        subtitle_frames = []
        
        # ä½¿ç”¨ OpenCV è¿›è¡Œå­—å¹•åŒºåŸŸæ£€æµ‹
        for i, frame_path in enumerate(frames):
            try:
                frame = cv2.imread(frame_path)
                if frame is None:
                    continue
                
                # è½¬æ¢ä¸ºç°åº¦å›¾
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹æ‰¾åˆ°å­—å¹•åŒºåŸŸ
                edges = cv2.Canny(gray, 50, 150)
                
                # æŸ¥æ‰¾è½®å»“
                contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                # è®¡ç®—å­—å¹•åŒºåŸŸé¢ç§¯
                subtitle_area = 0
                for contour in contours:
                    area = cv2.contourArea(contour)
                    if area > 100:  # è¿‡æ»¤å°åŒºåŸŸ
                        subtitle_area += area
                
                # å¦‚æœå­—å¹•åŒºåŸŸå æ¯”è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯å­—å¹•å¸§
                frame_area = frame.shape[0] * frame.shape[1]
                if subtitle_area > frame_area * 0.01:  # 1% é˜ˆå€¼
                    timestamp = i * 2.0  # å‡è®¾æ¯å¸§é—´éš”2ç§’
                    
                    subtitle_frame = SubtitleFrame(
                        frame_path=frame_path,
                        timestamp=timestamp,
                        frame_index=i,
                        confidence=min(1.0, subtitle_area / frame_area)
                    )
                    subtitle_frames.append(subtitle_frame)
                
            except Exception as e:
                self.logger.warning(f"å¤„ç†å¸§ {frame_path} æ—¶å‡ºé”™: {e}")
                continue
        
        self.logger.info(f"ğŸ” å­—å¹•å¸§æ£€æµ‹å®Œæˆï¼Œæ‰¾åˆ° {len(subtitle_frames)} å¸§")
        return subtitle_frames
    
    def _ocr_with_qwen3_parallel(self, subtitle_frames: List[SubtitleFrame], 
                               task_dir: Path) -> List[Dict[str, Any]]:
        """å¹¶è¡Œ OCR è¯†åˆ«"""
        ocr_results = []
        
        def process_single_frame(subtitle_frame: SubtitleFrame) -> Optional[Dict[str, Any]]:
            """å¤„ç†å•ä¸ªå¸§"""
            try:
                with open(subtitle_frame.frame_path, 'rb') as f:
                    image_data = f.read()
                
                # æ„å»ºå¢å¼ºçš„æç¤ºè¯
                prompt = f"""
                è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡æœ¬ï¼Œè¦æ±‚ï¼š
                1. å¦‚æœæœ‰å¤šè¡Œå­—å¹•ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
                2. è¯†åˆ«è¯­è¨€ç±»å‹ï¼ˆä¸­æ–‡/è‹±æ–‡/å…¶ä»–ï¼‰
                3. åˆ†æå­—å¹•çš„æƒ…æ„Ÿè‰²å½©ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰
                4. åªè¿”å›å­—å¹•æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–è§£é‡Š
                5. å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œè¯·è¿”å›"æ— å­—å¹•"
                """
                
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                }
                
                data = {
                    "model": "qwen3-omni-flash-realtime",
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_data.hex()}"
                                    }
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                }
                            ]
                        }
                    ],
                    "max_tokens": 500
                }
                
                response = requests.post(self.base_url, headers=headers, json=data, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    content = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                    
                    return {
                        "frame_index": subtitle_frame.frame_index,
                        "timestamp": subtitle_frame.timestamp,
                        "text": content,
                        "confidence": subtitle_frame.confidence,
                        "frame_path": subtitle_frame.frame_path
                    }
                else:
                    self.logger.warning(f"OCR è¯·æ±‚å¤±è´¥: {response.status_code}")
                    return None
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†å¸§ {subtitle_frame.frame_path} æ—¶å‡ºé”™: {e}")
                return None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_frame = {
                executor.submit(process_single_frame, frame): frame 
                for frame in subtitle_frames
            }
            
            for future in as_completed(future_to_frame):
                result = future.result()
                if result:
                    ocr_results.append(result)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(0.1)
        
        self.logger.info(f"ğŸ“ OCR è¯†åˆ«å®Œæˆï¼Œå¤„ç†äº† {len(ocr_results)} å¸§")
        return ocr_results
    
    def _enhance_subtitles_with_emotion(self, ocr_results: List[Dict[str, Any]], 
                                      task_dir: Path) -> List[SubtitleSegment]:
        """å¢å¼ºå­—å¹•æƒ…æ„Ÿåˆ†æ"""
        enhanced_subtitles = []
        
        # æŒ‰æ—¶é—´æ’åº
        ocr_results.sort(key=lambda x: x['timestamp'])
        
        for i, result in enumerate(ocr_results):
            if result['text'] == 'æ— å­—å¹•':
                continue
            
            # åŸºç¡€å­—å¹•ä¿¡æ¯
            start_time = result['timestamp']
            end_time = start_time + 3.0  # å‡è®¾æ˜¾ç¤º3ç§’
            
            # æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            emotion_tags = self._analyze_emotion(result['text'])
            
            subtitle_segment = SubtitleSegment(
                start_time=start_time,
                end_time=end_time,
                text=result['text'],
                confidence=result['confidence'],
                emotion_tags=emotion_tags
            )
            
            enhanced_subtitles.append(subtitle_segment)
        
        # åˆå¹¶ç›¸é‚»çš„ç›¸ä¼¼å­—å¹•
        enhanced_subtitles = self._merge_similar_subtitles(enhanced_subtitles)
        
        self.logger.info(f"ğŸ­ æƒ…æ„Ÿåˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(enhanced_subtitles)} æ¡å­—å¹•")
        return enhanced_subtitles
    
    def _analyze_emotion(self, text: str) -> List[str]:
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ"""
        emotions = []
        
        # ç§¯ææƒ…æ„Ÿè¯æ±‡
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æˆåŠŸ', 'å¿«ä¹', 'å¼€å¿ƒ', 'æ»¡æ„', 'èµ']
        # æ¶ˆææƒ…æ„Ÿè¯æ±‡
        negative_words = ['å', 'å·®', 'å¤±è´¥', 'éš¾è¿‡', 'ç”Ÿæ°”', 'ä¸æ»¡', 'æ‰¹è¯„']
        
        text_lower = text.lower()
        
        if any(word in text_lower for word in positive_words):
            emotions.append('ç§¯æ')
        elif any(word in text_lower for word in negative_words):
            emotions.append('æ¶ˆæ')
        else:
            emotions.append('ä¸­æ€§')
        
        return emotions
    
    def _merge_similar_subtitles(self, subtitles: List[SubtitleSegment]) -> List[SubtitleSegment]:
        """åˆå¹¶ç›¸ä¼¼çš„å­—å¹•"""
        if not subtitles:
            return []
        
        merged = [subtitles[0]]
        
        for current in subtitles[1:]:
            last = merged[-1]
            
            # å¦‚æœæ—¶é—´ç›¸è¿‘ä¸”å†…å®¹ç›¸ä¼¼ï¼Œåˆå¹¶
            if (current.start_time - last.end_time < 1.0 and 
                current.text in last.text or last.text in current.text):
                # åˆå¹¶å­—å¹•
                merged[-1] = SubtitleSegment(
                    start_time=last.start_time,
                    end_time=current.end_time,
                    text=f"{last.text} {current.text}",
                    confidence=(last.confidence + current.confidence) / 2,
                    emotion_tags=list(set(last.emotion_tags + current.emotion_tags))
                )
            else:
                merged.append(current)
        
        return merged
    
    def _generate_enhanced_srt(self, subtitles: List[SubtitleSegment], output_path: str):
        """ç”Ÿæˆå¢å¼ºç‰ˆ SRT æ–‡ä»¶"""
        srt_content = ""
        
        for i, subtitle in enumerate(subtitles):
            start_time = self._format_time_enhanced(subtitle.start_time)
            end_time = self._format_time_enhanced(subtitle.end_time)
            
            srt_content += f"{i + 1}\n"
            srt_content += f"{start_time} --> {end_time}\n"
            srt_content += f"{subtitle.text}\n"
            
            # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾ä½œä¸ºæ³¨é‡Š
            if subtitle.emotion_tags:
                srt_content += f"// æƒ…æ„Ÿ: {', '.join(subtitle.emotion_tags)}\n"
            
            srt_content += "\n"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        self.logger.info(f"ğŸ“„ SRT æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
    
    def _format_time_enhanced(self, seconds: float) -> str:
        """å¢å¼ºç‰ˆæ—¶é—´æ ¼å¼åŒ–"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def _cleanup_temp_files(self, task_dir: Path):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            shutil.rmtree(task_dir)
            self.logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {task_dir}")
        except Exception as e:
            self.logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    extractor = EnhancedQwen3SubtitleExtractor(api_key)
    
    # ç¤ºä¾‹ç”¨æ³•
    video_path = "example_video.mp4"
    output_path = "output_subtitles_enhanced.srt"
    task_id = "demo_task"
    
    if os.path.exists(video_path):
        start_time = time.time()
        result = extractor.extract_subtitles_from_video(
            video_path, output_path, task_id
        )
        
        if result["success"]:
            print(f"ğŸ‰ å­—å¹•æå–æˆåŠŸï¼")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   - æ€»å¸§æ•°: {result['total_frames']}")
            print(f"   - å­—å¹•å¸§æ•°: {result['subtitle_frames']}")
            print(f"   - å­—å¹•æ¡æ•°: {result['subtitles_count']}")
            print(f"   - å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
        else:
            print(f"âŒ å­—å¹•æå–å¤±è´¥: {result['error']}")
    else:
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

if __name__ == "__main__":
    main()