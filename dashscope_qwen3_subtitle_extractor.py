#!/usr/bin/env python3
"""
åŸºäº DashScope SDK çš„ Qwen3-Omni-Flash å­—å¹•æå–å™¨
ä½¿ç”¨å®˜æ–¹ SDK å®ç°æ›´å¯é çš„å¤šæ¨¡æ€å¤„ç†
"""

import os
import cv2
import json
import base64
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

import dashscope
from dashscope import TextGeneration, ImageUnderstanding

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class SubtitleFrame:
    """å­—å¹•å¸§æ•°æ®ç»“æ„"""
    frame_path: str
    timestamp: float
    frame_index: int
    confidence: float = 0.0

@dataclass
class SubtitleSegment:
    """å­—å¹•ç‰‡æ®µæ•°æ®ç»“æ„"""
    start_time: float
    end_time: float
    text: str
    confidence: float = 0.0
    emotion_tags: List[str] = None

class DashScopeQwen3SubtitleExtractor:
    """åŸºäº DashScope SDK çš„ Qwen3-Omni-Flash å­—å¹•æå–å™¨"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–å­—å¹•æå–å™¨"""
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        
        dashscope.api_key = self.api_key
        
        # é…ç½®å‚æ•°
        self.max_workers = 4
        self.request_timeout = 30
        self.retry_attempts = 3
        self.retry_delay = 1.0
        
        # ä¸´æ—¶ç›®å½•
        self.temp_dir = Path("temp")
        self.temp_dir.mkdir(exist_ok=True)
        
        logger.info("DashScope Qwen3 å­—å¹•æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_subtitles_from_video(self, video_path: str, output_path: str, 
                                   task_id: str = None) -> Dict[str, Any]:
        """
        ä»è§†é¢‘ä¸­æå–å­—å¹•
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡º SRT æ–‡ä»¶è·¯å¾„
            task_id: ä»»åŠ¡ID
            
        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
            
            # 1. æå–è§†é¢‘å¸§
            frames = self._extract_frames(video_path)
            logger.info(f"ğŸ“¸ æå–äº† {len(frames)} å¸§")
            
            # 2. æ£€æµ‹å­—å¹•å¸§
            subtitle_frames = self._detect_subtitle_frames(frames)
            logger.info(f"ğŸ” æ£€æµ‹åˆ° {len(subtitle_frames)} å­—å¹•å¸§")
            
            # 3. ä½¿ç”¨ DashScope è¿›è¡Œ OCR è¯†åˆ«
            ocr_results = self._ocr_with_dashscope(subtitle_frames)
            
            # 4. æƒ…æ„Ÿåˆ†æå’Œå­—å¹•ä¼˜åŒ–
            enhanced_subtitles = self._enhance_subtitles_with_emotion(ocr_results)
            
            # 5. ç”Ÿæˆ SRT æ–‡ä»¶
            self._generate_enhanced_srt(enhanced_subtitles, output_path)
            
            # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files()
            
            result = {
                "success": True,
                "video_path": video_path,
                "output_path": output_path,
                "total_frames": len(frames),
                "subtitle_frames": len(subtitle_frames),
                "subtitles_count": len(enhanced_subtitles),
                "processing_time": time.time() - start_time,
                "task_id": task_id
            }
            
            logger.info(f"âœ… å­—å¹•æå–å®Œæˆ: {output_path}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å­—å¹•æå–å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "video_path": video_path,
                "task_id": task_id
            }
    
    def _extract_frames(self, video_path: str, fps: int = 1) -> List[str]:
        """æå–è§†é¢‘å¸§"""
        frames = []
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        frame_count = 0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps_video = cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps_video:.2f} FPS")
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # æŒ‰æŒ‡å®š fps é‡‡æ ·
            sample_rate = max(1, int(fps_video / fps))
            if frame_count % sample_rate == 0:
                frame_path = self.temp_dir / f"frame_{frame_count}.jpg"
                cv2.imwrite(str(frame_path), frame)
                frames.append(str(frame_path))
            
            frame_count += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if frame_count % 100 == 0:
                progress = (frame_count / total_frames) * 100
                logger.info(f"ğŸ“Š å¸§æå–è¿›åº¦: {progress:.1f}%")
        
        cap.release()
        return frames
    
    def _detect_subtitle_frames(self, frames: List[str]) -> List[SubtitleFrame]:
        """æ£€æµ‹åŒ…å«å­—å¹•çš„å¸§"""
        subtitle_frames = []
        
        for i, frame_path in enumerate(frames):
            try:
                frame = cv2.imread(frame_path)
                if frame is None:
                    continue
                
                # è½¬æ¢ä¸ºç°åº¦å›¾
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹æ‰¾åˆ°å­—å¹•åŒºåŸŸ
                edges = cv2.Canny(gray, 50, 150)
                
                # æŸ¥æ‰¾è½®å»“
                contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                # è®¡ç®—å­—å¹•åŒºåŸŸé¢ç§¯
                subtitle_area = 0
                for contour in contours:
                    area = cv2.contourArea(contour)
                    if area > 100:  # è¿‡æ»¤å°åŒºåŸŸ
                        subtitle_area += area
                
                # å¦‚æœå­—å¹•åŒºåŸŸå æ¯”è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯å­—å¹•å¸§
                frame_area = frame.shape[0] * frame.shape[1]
                if subtitle_area > frame_area * 0.01:  # 1% é˜ˆå€¼
                    timestamp = i * 2.0  # å‡è®¾æ¯å¸§é—´éš”2ç§’
                    
                    subtitle_frame = SubtitleFrame(
                        frame_path=frame_path,
                        timestamp=timestamp,
                        frame_index=i,
                        confidence=min(1.0, subtitle_area / frame_area)
                    )
                    subtitle_frames.append(subtitle_frame)
                
            except Exception as e:
                logger.warning(f"å¤„ç†å¸§ {frame_path} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"ğŸ” å­—å¹•å¸§æ£€æµ‹å®Œæˆï¼Œæ‰¾åˆ° {len(subtitle_frames)} å¸§")
        return subtitle_frames
    
    def _ocr_with_dashscope(self, subtitle_frames: List[SubtitleFrame]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ DashScope è¿›è¡Œ OCR è¯†åˆ«"""
        ocr_results = []
        
        for i, subtitle_frame in enumerate(subtitle_frames):
            try:
                # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
                with open(subtitle_frame.frame_path, 'rb') as f:
                    image_data = f.read()
                
                image_base64 = base64.b64encode(image_data).decode('utf-8')
                
                # æ„å»ºæç¤ºè¯
                prompt = f"""
                è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡æœ¬ï¼Œè¦æ±‚ï¼š
                1. å¦‚æœæœ‰å¤šè¡Œå­—å¹•ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
                2. è¯†åˆ«è¯­è¨€ç±»å‹ï¼ˆä¸­æ–‡/è‹±æ–‡/å…¶ä»–ï¼‰
                3. åˆ†æå­—å¹•çš„æƒ…æ„Ÿè‰²å½©ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰
                4. åªè¿”å›å­—å¹•æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–è§£é‡Š
                5. å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œè¯·è¿”å›"æ— å­—å¹•"
                """
                
                # è°ƒç”¨ DashScope æ–‡æœ¬ç”Ÿæˆ API
                response = TextGeneration.call(
                    model='qwen3-omni-flash-realtime',
                    messages=[
                        {
                            'role': 'user',
                            'content': [
                                {
                                    'image': f'data:image/jpeg;base64,{image_base64}'
                                },
                                {
                                    'text': prompt
                                }
                            ]
                        }
                    ],
                    parameters={
                        'max_tokens': 500,
                        'temperature': 0.1
                    }
                )
                
                if response.status_code == 200:
                    content = response.output.choices[0].message.content.strip()
                    
                    ocr_result = {
                        "frame_index": subtitle_frame.frame_index,
                        "timestamp": subtitle_frame.timestamp,
                        "text": content,
                        "confidence": subtitle_frame.confidence,
                        "frame_path": subtitle_frame.frame_path
                    }
                    ocr_results.append(ocr_result)
                else:
                    logger.warning(f"OCR è¯·æ±‚å¤±è´¥: {response.status_code}")
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"å¤„ç†å¸§ {subtitle_frame.frame_path} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"ğŸ“ OCR è¯†åˆ«å®Œæˆï¼Œå¤„ç†äº† {len(ocr_results)} å¸§")
        return ocr_results
    
    def _enhance_subtitles_with_emotion(self, ocr_results: List[Dict[str, Any]]) -> List[SubtitleSegment]:
        """å¢å¼ºå­—å¹•æƒ…æ„Ÿåˆ†æ"""
        enhanced_subtitles = []
        
        # æŒ‰æ—¶é—´æ’åº
        ocr_results.sort(key=lambda x: x['timestamp'])
        
        for i, result in enumerate(ocr_results):
            if result['text'] == 'æ— å­—å¹•':
                continue
            
            # åŸºç¡€å­—å¹•ä¿¡æ¯
            start_time = result['timestamp']
            end_time = start_time + 3.0  # å‡è®¾æ˜¾ç¤º3ç§’
            
            # æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            emotion_tags = self._analyze_emotion(result['text'])
            
            subtitle_segment = SubtitleSegment(
                start_time=start_time,
                end_time=end_time,
                text=result['text'],
                confidence=result['confidence'],
                emotion_tags=emotion_tags
            )
            
            enhanced_subtitles.append(subtitle_segment)
        
        # åˆå¹¶ç›¸é‚»çš„ç›¸ä¼¼å­—å¹•
        enhanced_subtitles = self._merge_similar_subtitles(enhanced_subtitles)
        
        logger.info(f"ğŸ­ æƒ…æ„Ÿåˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(enhanced_subtitles)} æ¡å­—å¹•")
        return enhanced_subtitles
    
    def _analyze_emotion(self, text: str) -> List[str]:
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ"""
        emotions = []
        
        # ç§¯ææƒ…æ„Ÿè¯æ±‡
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æˆåŠŸ', 'å¿«ä¹', 'å¼€å¿ƒ', 'æ»¡æ„', 'èµ', 'çˆ±', 'å¸Œæœ›']
        # æ¶ˆææƒ…æ„Ÿè¯æ±‡
        negative_words = ['å', 'å·®', 'å¤±è´¥', 'éš¾è¿‡', 'ç”Ÿæ°”', 'ä¸æ»¡', 'æ‰¹è¯„', 'è®¨åŒ', 'ç—›è‹¦', 'ç»æœ›']
        
        text_lower = text.lower()
        
        if any(word in text_lower for word in positive_words):
            emotions.append('ç§¯æ')
        elif any(word in text_lower for word in negative_words):
            emotions.append('æ¶ˆæ')
        else:
            emotions.append('ä¸­æ€§')
        
        return emotions
    
    def _merge_similar_subtitles(self, subtitles: List[SubtitleSegment]) -> List[SubtitleSegment]:
        """åˆå¹¶ç›¸ä¼¼çš„å­—å¹•"""
        if not subtitles:
            return []
        
        merged = [subtitles[0]]
        
        for current in subtitles[1:]:
            last = merged[-1]
            
            # å¦‚æœæ—¶é—´ç›¸è¿‘ä¸”å†…å®¹ç›¸ä¼¼ï¼Œåˆå¹¶
            if (current.start_time - last.end_time < 1.0 and 
                (current.text in last.text or last.text in current.text)):
                # åˆå¹¶å­—å¹•
                merged[-1] = SubtitleSegment(
                    start_time=last.start_time,
                    end_time=current.end_time,
                    text=f"{last.text} {current.text}",
                    confidence=(last.confidence + current.confidence) / 2,
                    emotion_tags=list(set(last.emotion_tags + current.emotion_tags))
                )
            else:
                merged.append(current)
        
        return merged
    
    def _generate_enhanced_srt(self, subtitles: List[SubtitleSegment], output_path: str):
        """ç”Ÿæˆå¢å¼ºç‰ˆ SRT æ–‡ä»¶"""
        srt_content = ""
        
        for i, subtitle in enumerate(subtitles):
            start_time = self._format_time_enhanced(subtitle.start_time)
            end_time = self._format_time_enhanced(subtitle.end_time)
            
            srt_content += f"{i + 1}\n"
            srt_content += f"{start_time} --> {end_time}\n"
            srt_content += f"{subtitle.text}\n"
            
            # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾ä½œä¸ºæ³¨é‡Š
            if subtitle.emotion_tags:
                srt_content += f"// æƒ…æ„Ÿ: {', '.join(subtitle.emotion_tags)}\n"
            
            srt_content += "\n"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        logger.info(f"ğŸ“„ SRT æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
    
    def _format_time_enhanced(self, seconds: float) -> str:
        """å¢å¼ºç‰ˆæ—¶é—´æ ¼å¼åŒ–"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
                logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    extractor = DashScopeQwen3SubtitleExtractor(api_key)
    
    # ç¤ºä¾‹ç”¨æ³•
    video_path = "example_video.mp4"
    output_path = "output_subtitles_dashscope.srt"
    task_id = "demo_task"
    
    if os.path.exists(video_path):
        result = extractor.extract_subtitles_from_video(
            video_path, output_path, task_id
        )
        
        if result["success"]:
            print(f"ğŸ‰ å­—å¹•æå–æˆåŠŸï¼")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   - æ€»å¸§æ•°: {result['total_frames']}")
            print(f"   - å­—å¹•å¸§æ•°: {result['subtitle_frames']}")
            print(f"   - å­—å¹•æ¡æ•°: {result['subtitles_count']}")
            print(f"   - å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
        else:
            print(f"âŒ å­—å¹•æå–å¤±è´¥: {result['error']}")
    else:
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

if __name__ == "__main__":
    main()