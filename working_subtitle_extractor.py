#!/usr/bin/env python3
"""
å·¥ä½œå­—å¹•æå–å™¨
åŸºäº DashScope API å’Œ Qwen æ¨¡å‹
"""

import os
import sys
import json
import time
import logging
import base64
import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WorkingSubtitleExtractor:
    """å·¥ä½œå­—å¹•æå–å™¨"""
    
    def __init__(self, api_key: str = "sk-88bf1bd605544d208c7338cb1989ab3e"):
        """åˆå§‹åŒ–å­—å¹•æå–å™¨"""
        self.api_key = api_key
        self.model = "qwen-plus"  # ä½¿ç”¨å·²ç»éªŒè¯å¯ç”¨çš„æ¨¡å‹
        self.setup_environment()
        
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        os.environ['DASHSCOPE_API_KEY'] = self.api_key
        
    def extract_frames_from_video(self, video_path: str, output_dir: str, 
                                 frame_interval: int = 30) -> List[str]:
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        logger.info(f"ğŸ¬ ä»è§†é¢‘ä¸­æå–å¸§: {video_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return []
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        logger.info(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
        logger.info(f"   - æ€»å¸§æ•°: {total_frames}")
        logger.info(f"   - å¸§ç‡: {fps:.2f} FPS")
        logger.info(f"   - æ—¶é•¿: {duration:.2f} ç§’")
        
        # æå–å¸§
        frame_paths = []
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            # æŒ‰é—´éš”æå–å¸§
            if frame_count % frame_interval == 0:
                frame_path = os.path.join(output_dir, f"frame_{frame_count:06d}.jpg")
                cv2.imwrite(frame_path, frame)
                frame_paths.append(frame_path)
                
            frame_count += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if frame_count % 100 == 0:
                progress = (frame_count / total_frames) * 100
                logger.info(f"ğŸ”„ å¤„ç†è¿›åº¦: {progress:.1f}%")
        
        cap.release()
        logger.info(f"âœ… æå–äº† {len(frame_paths)} å¸§åˆ°: {output_dir}")
        return frame_paths
    
    def extract_text_from_image(self, image_path: str) -> Optional[str]:
        """ä»å›¾ç‰‡ä¸­æå–æ–‡æœ¬"""
        logger.info(f"ğŸ“¸ ä»å›¾ç‰‡ä¸­æå–æ–‡æœ¬: {image_path}")
        
        try:
            import dashscope
            from dashscope import Generation
            
            # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
            with open(image_path, 'rb') as f:
                image_data = f.read()
            
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # è°ƒç”¨ DashScope API
            response = Generation.call(
                model=self.model,
                messages=[
                    {
                        'role': 'user',
                        'content': [
                            {
                                'image': f'data:image/jpeg;base64,{image_base64}'
                            },
                            {
                                'text': 'è¯·ä»”ç»†è¯†åˆ«å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡æœ¬ã€‚å¦‚æœæœ‰å¤šè¡Œå­—å¹•ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ã€‚åªè¿”å›å­—å¹•æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœå›¾ç‰‡ä¸­æ²¡æœ‰å­—å¹•æ–‡æœ¬ï¼Œè¯·è¿”å›"æ— å­—å¹•"ã€‚'
                            }
                        ]
                    }
                ],
                parameters={
                    'max_tokens': 200,
                    'temperature': 0.1
                }
            )
            
            if response.status_code == 200:
                if hasattr(response, 'output') and response.output:
                    if hasattr(response.output, 'text') and response.output.text:
                        text = response.output.text.strip()
                        logger.info(f"ğŸ“ è¯†åˆ«ç»“æœ: {text}")
                        return text
                    else:
                        logger.error(f"âŒ å“åº”ç»“æ„å¼‚å¸¸: text ä¸ºç©º")
                        return None
                else:
                    logger.error(f"âŒ å“åº”ç»“æ„å¼‚å¸¸: output ä¸ºç©º")
                    return None
            else:
                logger.error(f"âŒ API è°ƒç”¨å¤±è´¥: {response.message}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æå–æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def process_frames_with_subtitles(self, frame_paths: List[str], 
                                   output_file: str) -> Dict[str, Any]:
        """å¤„ç†åŒ…å«å­—å¹•çš„å¸§"""
        logger.info(f"ğŸ”„ å¤„ç† {len(frame_paths)} å¸§...")
        
        results = []
        processed_count = 0
        subtitle_count = 0
        
        for i, frame_path in enumerate(frame_paths):
            logger.info(f"ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(frame_paths)} å¸§...")
            
            # æå–æ–‡æœ¬
            text = self.extract_text_from_image(frame_path)
            
            if text and text != "æ— å­—å¹•":
                # è®¡ç®—æ—¶é—´æˆ³ï¼ˆå‡è®¾æ¯å¸§é—´éš” 1 ç§’ï¼‰
                timestamp = i * 1.0  # ç®€åŒ–å¤„ç†
                
                results.append({
                    'timestamp': timestamp,
                    'text': text,
                    'frame_path': frame_path
                })
                
                subtitle_count += 1
                logger.info(f"âœ… å‘ç°å­—å¹•: {text}")
            
            processed_count += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if processed_count % 10 == 0:
                progress = (processed_count / len(frame_paths)) * 100
                logger.info(f"ğŸ”„ å¤„ç†è¿›åº¦: {progress:.1f}%")
        
        # ç”Ÿæˆ SRT æ–‡ä»¶
        self.generate_srt_file(results, output_file)
        
        return {
            'success': True,
            'processed_frames': processed_count,
            'subtitle_frames': subtitle_count,
            'output_file': output_file,
            'results': results
        }
    
    def generate_srt_file(self, results: List[Dict], output_file: str):
        """ç”Ÿæˆ SRT æ–‡ä»¶"""
        logger.info(f"ğŸ“ ç”Ÿæˆ SRT æ–‡ä»¶: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, result in enumerate(results):
                start_time = result['timestamp']
                end_time = start_time + 3.0  # å‡è®¾å­—å¹•æ˜¾ç¤º 3 ç§’
                text = result['text']
                
                # æ ¼å¼åŒ–æ—¶é—´
                start_time_str = self.format_time(start_time)
                end_time_str = self.format_time(end_time)
                
                # å†™å…¥ SRT æ ¼å¼
                f.write(f"{i+1}\n")
                f.write(f"{start_time_str} --> {end_time_str}\n")
                f.write(f"{text}\n\n")
        
        logger.info(f"âœ… SRT æ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
    
    def format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def extract_subtitles_from_video(self, video_path: str, 
                                   output_file: str) -> Dict[str, Any]:
        """ä»è§†é¢‘ä¸­æå–å­—å¹•"""
        logger.info(f"ğŸ¬ å¼€å§‹ä»è§†é¢‘ä¸­æå–å­—å¹•: {video_path}")
        
        start_time = time.time()
        
        # æå–å¸§
        frames_dir = os.path.join(os.path.dirname(output_file), "frames")
        frame_paths = self.extract_frames_from_video(video_path, frames_dir)
        
        if not frame_paths:
            return {
                'success': False,
                'error': 'æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§'
            }
        
        # å¤„ç†å¸§
        result = self.process_frames_with_subtitles(frame_paths, output_file)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time
        
        result['processing_time'] = processing_time
        
        logger.info(f"âœ… å­—å¹•æå–å®Œæˆ:")
        logger.info(f"   - å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        logger.info(f"   - å¤„ç†å¸§æ•°: {result['processed_frames']}")
        logger.info(f"   - å­—å¹•å¸§æ•°: {result['subtitle_frames']}")
        logger.info(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        return result

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å·¥ä½œå­—å¹•æå–å™¨")
    print("=" * 40)
    
    # åˆ›å»ºæå–å™¨
    extractor = WorkingSubtitleExtractor()
    
    # æµ‹è¯•è§†é¢‘æ–‡ä»¶
    test_video = "test_video.mp4"
    if not os.path.exists(test_video):
        print(f"âš ï¸  æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {test_video}")
        print("ğŸ’¡ è·³è¿‡æµ‹è¯•")
        return
    
    # æ‰§è¡Œå­—å¹•æå–
    result = extractor.extract_subtitles_from_video(test_video, "test_output.srt")
    
    if result["success"]:
        print("ğŸ‰ å­—å¹•æå–æˆåŠŸï¼")
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
        print(f"   - å¤„ç†å¸§æ•°: {result.get('processed_frames', 0)}")
        print(f"   - å­—å¹•å¸§æ•°: {result.get('subtitle_frames', 0)}")
        print(f"   - è¾“å‡ºæ–‡ä»¶: {result.get('output_file', 'N/A')}")
    else:
        print(f"âŒ å­—å¹•æå–å¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()